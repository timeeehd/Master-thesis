{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407a7550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f8a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0346547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed to a fixed value to get reproducible results \n",
    "torch.manual_seed(42)\n",
    "# Download the pre-trained GPT-Neo model's tokenizer\n",
    "# Add the custom tokens denoting the beginning and the end \n",
    "# of the sequence and a special token for padding\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\",    \n",
    "#                             bos_token=\"<|startoftext|>\",\n",
    "#                             eos_token=\"<|endoftext|>\",\n",
    "#                             pad_token=\"<|pad|>\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"models/tokenizer/neo\")\n",
    "# special_tokens_dict = {\n",
    "#         \"bos_token\": \"<BOS>\",\n",
    "#         \"eos_token\": \"<EOS>\",\n",
    "#         \"pad_token\": \"<PAD>\",\n",
    "#         \"additional_special_tokens\": [\n",
    "#             \"<endprompt>\",\n",
    "#         ],\n",
    "#     }\n",
    "\n",
    "# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# Download the pre-trained GPT-Neo model and transfer it to the GPU\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"models/checkpoint-40000\").cuda()\n",
    "# Resize the token embeddings because we've just added 3 new tokens \n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e6abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Fairy_tales_combined (1).txt', \"r\", encoding='utf-8-sig') as file:\n",
    "    data = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a65314",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetflixDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for txt in txt_list:\n",
    "            # Encode the descriptions using the GPT-Neo tokenizer\n",
    "            encodings_dict = tokenizer(txt,\n",
    "                                        truncation=True,\n",
    "                                        max_length=max_length, \n",
    "                                        padding=\"max_length\")\n",
    "            input_ids = torch.tensor(encodings_dict['input_ids'])    \n",
    "            self.input_ids.append(input_ids)\n",
    "            mask = torch.tensor(encodings_dict['attention_mask'])\n",
    "            self.attn_masks.append(mask)\n",
    "    \n",
    "    def __len__(self):\n",
    "     return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "     return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e34037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = max([len(tokenizer.encode(row)) for row in data])\n",
    "\n",
    "dataset = NetflixDataset(data, tokenizer, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9893985a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9183"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d86a76d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50257, 10970,   367, 24805,    56,  4810,  1268,  5222,   220,  3334,\n",
       "          2029,   262,  1748,   837,   319,   257,  7331,  5721,   837,  6204,\n",
       "           262, 15207,   286,   262, 14628,   220,  9005,   764, 50260,  1544,\n",
       "           373,   308, 46158,   477,   625,   351,  7888,  5667,   286,  3734,\n",
       "          3869,   837,   329,  2951,   220,   339,   550,   734,  6016,   473,\n",
       "           381,    71,  2387,   837,   290,   257,  1588,  2266, 43506,  1278,\n",
       "          6972,   319,   465,   220,  8429,   532,   289,  2326,   764,   220,\n",
       "           679,   373,   845,   881, 29382,  5600,   764,   366,   679,   318,\n",
       "           355,  4950,   355,   257,  6193, 21517,   837,   366,   220, 24998,\n",
       "           530,   286,   262,  8329,  3545, 20346,   669,   508, 16555,   284,\n",
       "          4461,   257,  8507,   329,   220,  1719, 17290, 18221,  2162,   366,\n",
       "           691,   407,  2407,   523,  4465,   837,   366,   339,  2087,   837,\n",
       "         33188,   220, 27380,   661,   815,   892,   683,  8593, 36112,   837,\n",
       "           543,   339,  1107,   373,   407,   764,   220,   366,  4162,  1275,\n",
       "           299,   470,   345,   307,   588,   262, 14628,  9005,  5633,   366,\n",
       "          1965,   257, 20586,  2802,   286,   607,   220,  1310,  2933,   508,\n",
       "           373, 13774,   329,   262,  8824,   764,   366,   383, 14628,  9005,\n",
       "          1239, 10625,   220,   286, 13774,   329,  1997,   764,   366,   220,\n",
       "           366,   314,   716,  9675,   612,   318,   617,   530,   287,   262,\n",
       "           995,   508,   318,  2407,  3772,   837,   366, 34042,   220,   257,\n",
       "         11679,   582,   355,   339, 50255,   379,   262,  7932, 15207,   764,\n",
       "           220,   366,   679,  3073,   655,   588,   281, 18304,   837,   366,\n",
       "           531,   262, 37575,  8990,   355,   484,  1625,   220,   503,   286,\n",
       "           262, 41756,   287,   511,  6016, 10153,  1616, 28050,  4730,   290,\n",
       "           511,  3424,   220,  2330,   279,  1437,   754,    82,   764,   220,\n",
       "           366,  1374,   466,   345,   760,  5633,   366,   531,   262, 30535,\n",
       "           605,  5599,   837,   366,   345,   423,  1239,  1775,   220,   530,\n",
       "           764,   366,   220,   366,  7900,  5145,   475,   356,   423,   837,\n",
       "           287,   674, 10625,   837,   366,  9373,   262,  1751,  2162,   290,\n",
       "           262,   220, 30535,   605,  5599, 32198,   290,  3114,   845,  6049,\n",
       "           837,   329,   339,   750,   407,   220, 14762,   286,  1751, 28368,\n",
       "           764,   220,  1881,  1755,   612, 13112,   625,   262,  1748,   257,\n",
       "          1310,  2451, 12154,   764,  2399,  2460,   550,   220,  3750,  1497,\n",
       "           284,  6365,  2237,  2745,   878,   837,   475,   339,   550,  9658,\n",
       "          2157,   837,   329,   339,   220,   373,   287,  1842,   351,   262,\n",
       "           749,  4950, 16105,   764,   679,   550,  1138,   607,  1903,   287,\n",
       "           262,   220,  6076,   355,   339,   373,  7348,   866,   262,  7850,\n",
       "           706,   257,  1263,  7872, 44400,   837,   290,   550,   220,   587,\n",
       "           523, 12725,   416,   607, 36808, 16139,   326,   339,   550,  5025,\n",
       "           284,  1561,   284,   220,   607,   764,   220,   366, 38451,   314,\n",
       "          1842,   345,  5633,   366,   531,   262,  2451, 12154,   837,   508,\n",
       "          8288,   284,  1282,   284,   262,   966,   379,   220,  1752,   837,\n",
       "           290,   262, 16105,   925,   683,   257,  1877,  9563,   764,  1406,\n",
       "           339, 13112,  2835,   290,  2835,   607,   837,   220, 15241,   262,\n",
       "          1660,   351,   465, 12098,   837,   290,  1642,  8465,   374, 27844,\n",
       "           764,   770,   373,   220,   465,  8028,  1056,   837,   290,   340,\n",
       "         15436,   477,   832,   262,  3931,   764,   220,   366,   632,   318,\n",
       "           257, 11441, 18231,   837,   366,   665, 36613,   262,   584,  2451,\n",
       "         47205,  2162,   366,   673,   468,   220,   645,  1637,   837,   290,\n",
       "          1290,  1165,   867,  2316,  2162,   366,   290,  5600,   262,  7850,\n",
       "           373,  2407,   220,  1336,   286,   797,  5379,   764,  3244,   837,\n",
       "           618,   262]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3629abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, \n",
    "                            [train_size, len(dataset) - train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aff5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I will pass the output directory where \n",
    "# the model predictions and checkpoints will be stored, \n",
    "# batch sizes for the training and validation steps, \n",
    "# and warmup_steps to gradually increase the learning rate\n",
    "training_args = TrainingArguments(output_dir=\"./models\",\n",
    "                                  num_train_epochs=10,\n",
    "                                  logging_steps=5000,\n",
    "                                  save_steps=5000,                                   \n",
    "                                  per_device_train_batch_size=2,\n",
    "                                  per_device_eval_batch_size=2,\n",
    "                                  warmup_steps=100,\n",
    "                                  weight_decay=0.01,  \n",
    "                                  logging_dir=\"./logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 8264\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 41320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37672' max='41320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37672/41320 2:18:29 < 13:24, 4.53 it/s, Epoch 9.12/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.440300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.790500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.664200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models\\checkpoint-5000\n",
      "Configuration saved in ./models\\checkpoint-5000\\config.json\n",
      "Model weights saved in ./models\\checkpoint-5000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models\\checkpoint-10000\n",
      "Configuration saved in ./models\\checkpoint-10000\\config.json\n",
      "Model weights saved in ./models\\checkpoint-10000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models\\checkpoint-15000\n",
      "Configuration saved in ./models\\checkpoint-15000\\config.json\n",
      "Model weights saved in ./models\\checkpoint-15000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models\\checkpoint-20000\n",
      "Configuration saved in ./models\\checkpoint-20000\\config.json\n",
      "Model weights saved in ./models\\checkpoint-20000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models\\checkpoint-25000\n",
      "Configuration saved in ./models\\checkpoint-25000\\config.json\n",
      "Model weights saved in ./models\\checkpoint-25000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models\\checkpoint-30000\n",
      "Configuration saved in ./models\\checkpoint-30000\\config.json\n",
      "Model weights saved in ./models\\checkpoint-30000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./models\\checkpoint-35000\n",
      "Configuration saved in ./models\\checkpoint-35000\\config.json\n",
      "Model weights saved in ./models\\checkpoint-35000\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, args=training_args,  \n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=val_dataset, \n",
    "                  # This custom collate function is necessary \n",
    "                  # to built batches of data\n",
    "                  data_collator=lambda data: \n",
    "              {\"input_ids\": torch.stack([f[0] for f in data]),       \n",
    "               \"attention_mask\": torch.stack([f[1] for f in data]),\n",
    "               \"labels\": torch.stack([f[0] for f in data])})\n",
    "# Start training process!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec1cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = tokenizer.encode(\n",
    "    f\" <BOS> MY FATHER MEETS THE CAT  <newline>  <newline>  <newline>  One cold rainy day when my father was a little boy , he met an old  <newline>  alley cat on his street . <endprompt> <EOS>\",\n",
    "    return_tensors=\"pt\").cuda()\n",
    "model = model.cuda()\n",
    "\n",
    "sample_outputs = model.generate(generated, do_sample=False, top_k=50, max_length=1024, top_p=0.95,\n",
    "                                temperature=0, num_return_sequences=0, repetition_penalty=1.1)\n",
    "# sample_outputs = model.generate(generated, max_length=50)\n",
    "predicted_text2 = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
    "print(predicted_text2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0019a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('models/tokenizer/neo/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
