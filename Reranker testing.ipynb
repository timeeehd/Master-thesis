{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae7fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e06752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words(text):\n",
    "    return re.split(r'[ ](?=[\\w])', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b2d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(text):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        text (str): raw text to split to sentences on end of sentences marks.\n",
    "    Returns:\n",
    "        List of sentences from text.\n",
    "    \"\"\"\n",
    "    # Multiple options Negative lookbehind to prevent splitting on Mr. or Mrs.\n",
    "    split_pattern = r'(?<!Mr)(?<!Mrs)[.!?;\"]+'\n",
    "    # Split on end of sentence, but keep the punctuation marks.\n",
    "    sentences = list(map(str.strip, re.sub(\n",
    "        split_pattern, r'\\g<0>[cut]', text.strip()).split('[cut]')))\n",
    "    # If the last sentence is ''\n",
    "    if len(sentences) > 1 and len(sentences[-1]) < 3:\n",
    "        sentences.pop()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "821423ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _readabilty(text, texts_sentences):\n",
    "    \"\"\"\n",
    "    Uses length of sentences and length of words.\n",
    "    Higher is for more advanced readers.\n",
    "    If text is sparse i.e. mostly new lines, and doesn't end with an eos -> add a negative cost.  \n",
    "    Args:\n",
    "        text (str): original text to return score for.\n",
    "        texts_sentences (list): text split to sentences. \n",
    "    \"\"\"\n",
    "    txt_words = split_words(text)\n",
    "    num_letters = sum(len(word) for word in txt_words)\n",
    "    num_words = len(txt_words)\n",
    "    num_sent = len(texts_sentences)\n",
    "\n",
    "    # check if a \"sparse\" sentence\n",
    "    if num_sent == 1:\n",
    "        new_line_threshold = 0 if num_words == 0 else num_words // 4\n",
    "        if texts_sentences[0].count('\\n') > new_line_threshold or not re.search(r'(?<![A-Z])[.!?;\"]+', texts_sentences[0]):\n",
    "            num_sent = 0\n",
    "\n",
    "    letters_per_word = -10 if num_words == 0 else num_letters/num_words\n",
    "    words_per_sentence = -10 if num_sent == 0 else num_words/num_sent\n",
    "    # 0.5 to weight words_per_sentence higher\n",
    "    return 0.5*letters_per_word + words_per_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3837f143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (from nltk) (2022.4.24)\n",
      "Requirement already satisfied: click in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (from click->nltk) (4.11.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (from importlib-metadata->click->nltk) (4.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (from importlib-metadata->click->nltk) (3.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tfede\\Documents\\GitHub\\Master-thesis\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tfede\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tfede\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\tfede\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Find synonyms, nltk.download('wordnet')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('sentiwordnet')\n",
    "from nltk.corpus import sentiwordnet as swn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c71e1e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sentiment_polarity(filtered_words):\n",
    "    \"\"\"\n",
    "    Returns a positive sentiment polarity in range 0 = negative/objective to 100 = positive of the entire text.\n",
    "    Uses SentiWordnet to compute the positiveness polarity of the words and average that value.\n",
    "    Based on https://nlpforhackers.io/sentiment-analysis-intro/\n",
    "\n",
    "    Args:\n",
    "        filtered_words (set): set of non-stop, non-punctuation words. \n",
    "    \"\"\"\n",
    "    # If  empty\n",
    "    if len(filtered_words) < 2:\n",
    "        return 0\n",
    "\n",
    "    POS_TAG_TO_WN = {'J': wn.ADJ, 'N': wn.NOUN, 'R': wn.ADV, 'V': wn.VERB}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    text_sentiment = []\n",
    "    tagged_words = pos_tag(filtered_words)\n",
    "\n",
    "    for word, tag in tagged_words:\n",
    "        wn_tag = POS_TAG_TO_WN.get(tag[0], None)\n",
    "        if not wn_tag:\n",
    "            continue\n",
    "\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "\n",
    "        synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "        if not synsets:\n",
    "            continue\n",
    "\n",
    "        # Take the most common of the synthsets\n",
    "        synset = synsets[0]\n",
    "        pos_sentiment = swn.senti_synset(synset.name()).pos_score(\n",
    "        ) - swn.senti_synset(synset.name()).neg_score()\n",
    "        text_sentiment.append(pos_sentiment)\n",
    "    return 0 if not text_sentiment else max(int(np.mean(text_sentiment)*100), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a828a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _simplicity(filtered_words_set):\n",
    "    \"\"\"\n",
    "    Fraction of most frequent words from generated text.\n",
    "    Args:\n",
    "        filtered_words_set (set): set of non-stop, non-punctuation words. \n",
    "    \"\"\"\n",
    "    SEVEN_PREC_MOST_FREQ_WORDS = {'six', 'month', 'drew', 'want', 'hands', 'staring', 'guests', 'goose', 'fitted', 'rope', 'grace', 'delightful', 'meg', 'peace', 'lovely', 'iron', 'dark', 'cloak', 'pictures', 'eaten', 'sake', 'hurt', 'soldiers', 'dragon', 'late', 'unusual', 'centre', 'shore', 'gloomy', 'burning', 'time', 'foreign', 'bride', 'show', 'disappeared', 'light', 'spirits', 'arose', 'larger', 'sunshine', 'paul', 'cries', 'nearest', 'refuse', 'cut', 'fun', 'naughty', 'ears', 'remember', 'filled', 'playing', 'ask', 'loud', 'suggested', 'husband', 'placed', 'proud', 'places', 'difficult', 'somebody', 'eat', 'fault', 'school', 'honor', 'maybe', 'faith', 'win', 'full', 'missed', 'big', 'pieces', 'asking', 'case', 'unto', 'wolf', 'jennings', 'color', 'riding', 'elder', 'torn', 'stay', 'monkeys', 'comes', 'largest', 'crew', 'receive', 'feast', 'vast', 'foot', 'office', 'cake', 'throughout', 'indians', 'trust', 'cheerful', 'building', 'star', 'apple', 'younger', 'henry', 'matters', 'surface', 'york', 'summer', 'nothing', 'mentioned', 'come', 'hope', 'certain', 'seized', 'folded', 'jack', 'absolutely', 'surrounded', 'noise', 'hardly', 'entrance', 'hold', 'pass', 'subject', 'three', 'coming', 'rolling', 'mouse', 'sense', 'alarmed', 'influence', 'day', 'beneath', 'ceased', 'minchin', 'wood', 'laughter', 'difficulty', 'merry', 'marionette', 'state', 'dangerous', 'fought', 'lesson', 'ones', 'upper', 'house', 'mighty', 'ever', 'fountain', 'path', 'loss', 'help', 'present', 'tore', 'beat', 'princes', 'company', 'knees', 'wilt', 'charles', 'finished', 'blame', 'doors', 'replied', 'wait', 'cruel', 'glass', 'chest', 'extraordinary', 'mad', 'explain', 'eldest', 'sank', 'shed', 'driven', 'fall', 'prove', 'rising', 'angry', 'pulling', 'field', 'grandmother', 'mysterious', 'tea', 'sought', 'natural', 'raised', 'learned', 'must', 'hook', 'cared', 'gained', 'monster', 'beth', 'nearly', 'gazing', 'heap', 'rabbit', 'monsieur', 'stars', 'said', 'soft', 'jolly', 'stiff', 'exceedingly', 'us', 'named', 'smoke', 'colia', 'hurried', 'discover', 'close', 'removed', 'explained', 'crept', 'cutting', 'sight', 'mere', 'leaf', 'world', 'witch', 'putting', 'awful', 'grandfather', 'daughters', 'mountain', 'amid', 'understood', 'whispered', 'anger', 'hole', 'hunting', 'pretended', 'evening', 'shouted', 'sword', 'animal', 'swam', 'instant', 'madame', 'chariot', 'fairy', 'eager', 'master', 'raise', 'hercules', 'splendid', 'good', 'pocket', 'betty', 'bowed', 'thanks', 'regular', 'sorry', 'showed', 'dish', 'actually', 'place', 'sperm', 'water', 'aglaya', 'needs', 'pink', 'tumbled', 'ancient', 'rosy', 'sadly', 'else', 'lighted', 'sort', 'wall', 'dreamed', 'warm', 'bid', 'elinor', 'ice', 'week', 'supper', 'wounded', 'sell', 'refused', 'cook', 'otherwise', 'shoulder', 'glanced', 'dropped', 'stand', 'pretend', 'friends', 'skin', 'keep', 'call', 'beard', 'drawn', 'drawing', 'lying', 'move', 'married', 'send', 'edward', 'possibly', 'horrid', 'think', 'courage', 'great', 'work', 'gently', 'formed', 'society', 'rather', 'amy', 'branches', 'hopes', 'precious', 'depths', 'names', 'anxious', 'saw', 'cow', 'blue', 'forgot', 'talking', 'road', 'lose', 'rajah', 'attack', 'follow', 'third', 'leaves', 'waves', 'hang', 'jo', 'fancied', 'treasure', 'head', 'whatever', 'devil', 'fully', 'buried', 'wet', 'gun', 'looked', 'brought', 'next', 'farther', 'decided', 'band', 'cheeks', 'steal', 'sent', 'tossed', 'hear', 'maiden', 'servant', 'ride', 'tiny', 'feared', 'wave', 'travelled', 'one', 'green', 'teach', 'fro', 'tell', 'cool', 'cunning', 'catch', 'merely', 'badger', 'stepped', 'shining', 'owner', 'observe', 'chanced', 'bottom', 'hero', 'heavy', 'land', 'grateful', 'strange', 'circumstances', 'dwarf', 'quick', 'miserable', 'note', 'ship', 'fallen', 'word', 'john', 'turned', 'inclined', 'rat', 'easy', 'legs', 'bring', 'empty', 'touching', 'drinking', 'bade', 'army', 'sure', 'fierce', 'presented', 'key', 'tender', 'nobody', 'directly', 'build', 'edge', 'hidden', 'experience', 'begun', 'attempt', 'miles', 'violent', 'running', 'firmly', 'kicked', 'lived', 'party', 'chamber', 'savage', 'usually', 'drove', 'concluded', 'arranged', 'baby', 'pick', 'fail', 'england', 'reading', 'wants', 'generally', 'red', 'working', 'creatures', 'flew', 'keen', 'person', 'toad', 'related', 'twelve', 'aunt', 'confidence', 'moved', 'took', 'horrible', 'leaning', 'confess', 'spring', 'standing', 'vessel', 'locked', 'scarecrow', 'drank', 'reckon', 'happen', 'received', 'group', 'lebedeff', 'sun', 'sons', 'welcome', 'youngest', 'solid', 'impossible', 'princess', 'sky', 'learning', 'tongue', 'silly', 'began', 'fell', 'following', 'upstairs', 'moon', 'twice', 'given', 'everywhere', 'together', 'bell', 'heat', 'rough', 'gentle', 'carefully', 'longer', 'months', 'none', 'delicate', 'rest', 'darted', 'say', 'alone', 'former', 'everyone', 'rested', 'paid', 'devoted', 'understand', 'nodded', 'winter', 'animals', 'o', 'camp', 'consider', 'demanded', 'odd', 'pull', 'read', 'sold', 'bite', 'innocent', 'waters', 'heard', 'rode', 'stretched', 'gay', 'peculiar', 'position', 'done', 'shoes', 'express', 'streets', 'believe', 'later', 'fact', 'sea', 'seemed', 'drops', 'heaven', 'sail', 'hunt', 'grant', 'hans', 'grave', 'private', 'pipe', 'clock', 'put', 'taught', 'travel', 'beginning', 'slightly', 'view', 'slowly', 'blew', 'kept', 'plenty', 'bought', 'mind', 'belong', 'slow', 'eggs', 'special', 'happily', 'yellow', 'two', 'due', 'clear', 'handsome', 'deeply', 'short', 'cast', 'belonged', 'bore', 'enter', 'spread', 'shut', 'fill', 'closely', 'excited', 'capable', 'arm', 'obtained', 'seven', 'earth', 'truly', 'silence', 'matter', 'box', 'prisoner', 'gates', 'approaching', 'situation', 'mamma', 'gathered', 'mile', 'lot', 'way', 'clouds', 'song', 'without', 'distant', 'shoulders', 'kiss', 'cottage', 'armed', 'within', 'trying', 'smile', 'got', 'gate', 'eagle', 'neither', 'stands', 'somewhat', 'walking', 'asked', 'stubb', 'agreed', 'forgive', 'charge', 'bad', 'sprang', 'quite', 'washed', 'tear', 'retired', 'river', 'except', 'scarcely', 'landed', 'object', 'attracted', 'cock', 'rid', 'row', 'weight', 'watch', 'many', 'solitary', 'table', 'deeper', 'whether', 'swear', 'nearer', 'touched', 'colin', 'pleased', 'dreadful', 'false', 'ozma', 'dig', 'dorothy', 'future', 'cost', 'possible', 'anybody', 'offered', 'thy', 'sitting', 'line', 'darkness', 'kissed', 'summoned', 'laying', 'led', 'spell', 'considered', 'uncle', 'accustomed', 'marriage', 'whenever', 'pay', 'died', 'treated', 'hate', 'join', 'amuse', 'cried', 'finest', 'walk', 'things', 'won', 'hastened', 'leaped', 'emerald', 'swimming', 'affection', 'pounds', 'guard', 'swung', 'desert', 'bright', 'cure', 'laughing', 'piece', 'faster', 'hair', 'older', 'indeed', 'remain', 'reason', 'various', 'stuff', 'waving', 'comfortable', 'rise', 'settled', 'fool', 'friend', 'earnestly', 'island', 'comfort', 'beloved', 'kind', 'stayed', 'tin', 'happiness', 'ground', 'liked', 'sometimes', 'ivan', 'personal', 'longed', 'thing', 'meeting', 'simple', 'always', 'hunter', 'roof', 'obey', 'hot', 'delighted', 'moment', 'struck', 'cause', 'chance', 'whole', 'discovered', 'roll', 'seat', 'kitchen', 'bottle', 'taking', 'dickon', 'important', 'young', 'forget', 'bearing', 'safe', 'soon', 'deep', 'turning', 'holy', 'pinocchio', 'repeated', 'lady', 'papa', 'ways', 'crown', 'right', 'thou', 'body', 'dog', 'somewhere', 'steps', 'clearly', 'wedding', 'grey', 'cloth', 'mine', 'character', 'kindly', 'fish', 'silk', 'amongst', 'apparently', 'thinks', 'horse', 'latter', 'dim', 'wear', 'found', 'hurry', 'distance', 'perceived', 'apples', 'appeared', 'creature', 'form', 'tired', 'acquaintance', 'pale', 'remained', 'corner', 'moving', 'five', 'bless', 'queen', 'reward', 'defarge', 'kingdom', 'hit', 'governor', 'rubbed', 'stream', 'danced', 'host', 'bull', 'pressed', 'picture', 'escaped', 'crow', 'hall', 'wishing', 'persuaded', 'turkey', 'mortal', 'difference', 'laughed', 'thank', 'hand', 'front', 'dost', 'study', 'fly', 'nest', 'inquired', 'hours', 'burned', 'fetch', 'city', 'just', 'mischief', 'gradually', 'hated', 'claim', 'telling', 'yonder', 'faint', 'papers', 'meet', 'quickly', 'smith', 'plain', 'thrust', 'ran', 'entire', 'unhappy', 'lay', 'boat', 'roared', 'forty', 'wise', 'excellent', 'listened', 'vanished', 'climbed', 'manner', 'unless', 'parents', 'gift', 'rocks', 'force', 'low', 'garden', 'door', 'much', 'gray', 'greatest', 'wretched', 'year', 'angel', 'returned', 'stuck', 'waited', 'little', 'held', 'interested', 'fiery', 'purpose', 'gazed', 'woke', 'especially', 'recovered', 'useful', 'terribly', 'sad', 'wherever', 'might', 'carriage', 'maid', 'around', 'left', 'partly', 'sensible', 'ashamed', 'ordered', 'hollow', 'last', 'value', 'may', 'notice', 'troubles', 'mountains', 'wondered', 'weather', 'night', 'remarkable', 'slip', 'grow', 'floor', 'man', 'divided', 'wizard', 'highly', 'marched', 'rose', 'climb', 'fox', 'lucy', 'becoming', 'rushing', 'however', 'acted', 'number', 'meaning', 'orders', 'means', 'step', 'mention', 'tie', 'flowers', 'forced', 'promise', 'hoped', 'let', 'lie', 'poor', 'god', 'bundle', 'sacred', 'meantime', 'either', 'evil', 'white', 'game', 'return', 'invited', 'examined', 'swallowed', 'brave', 'shone', 'doctor', 'pointed', 'leg', 'deal', 'excuse', 'greater', 'yard', 'knock', 'accepted', 'share', 'wearing', 'proved', 'leading', 'still', 'books', 'seek', 'dying', 'higher', 'lion', 'flock', 'cross', 'favorite', 'laugh', 'see', 'houses', 'painful', 'stronger', 'adventure', 'parts', 'sat', 'bread', 'appear', 'sudden', 'proper', 'reach', 'worked', 'turns', 'memory', 'among', 'appearance', 'men', 'give', 'listen', 'heels', 'keeping', 'tsar', 'better', 'lead', 'point', 'best', 'guess', 'live', 'lift', 'leaving', 'write', 'shape', 'lifted', 'intention', 'blowing', 'stories', 'woman', 'windows', 'needed', 'bird', 'thither', 'cover', 'tears', 'allowed', 'wound', 'singing', 'lower', 'suppose', 'rain', 'room', 'hour', 'trouble', 'threw', 'tells', 'tall', 'top', 'sweet', 'fighting', 'kill', 'already', 'jason', 'tree', 'united', 'behind', 'shown', 'almost', 'danger', 'native', 'delicious', 'basket', 'dermat', 'leave', 'utter', 'pleasant', 'everything', 'wrote', 'knight', 'becky', 'vain', 'believed', 'bill', 'peter', 'music', 'ka', 'address', 'prepared', 'stupid', 'perfectly', 'admit', 'played',\n",
    "                              'beside', 'finally', 'wept', 'familiar', 'suffered', 'everybody', 'similar', 'though', 'fled', 'passing', 'law', 'imagine', 'succeeded', 'alive', 'usual', 'gives', 'battle', 'thoroughly', 'rooms', 'gone', 'followed', 'carrying', 'effort', 'rich', 'bent', 'hastily', 'prison', 'square', 'hid', 'along', 'eight', 'eagerly', 'fond', 'since', 'dared', 'trembling', 'bare', 'smooth', 'away', 'loaded', 'south', 'thought', 'boots', 'black', 'busy', 'stone', 'van', 'presence', 'sounded', 'bound', 'pleasure', 'idea', 'wooden', 'death', 'ordinary', 'willing', 'original', 'arms', 'added', 'spare', 'losing', 'told', 'money', 'fair', 'quarter', 'words', 'hannah', 'voice', 'knife', 'getting', 'peeped', 'free', 'sees', 'street', 'dug', 'monkey', 'frightened', 'grown', 'dogs', 'weary', 'fellow', 'pulled', 'finger', 'dwelt', 'resolved', 'woods', 'pure', 'according', 'tail', 'loving', 'drive', 'habit', 'magician', 'thinking', 'admitted', 'jumped', 'joy', 'play', 'kinds', 'curiosity', 'returning', 'towards', 'size', 'lucky', 'major', 'sailing', 'interest', 'name', 'days', 'declared', 'spoken', 'plan', 'palace', 'castle', 'trembled', 'court', 'chair', 'equal', 'ahab', 'known', 'announced', 'passage', 'old', 'run', 'hoping', 'rush', 'lofty', 'talk', 'powerful', 'truth', 'george', 'attend', 'fat', 'fate', 'clothes', 'news', 'thief', 'taken', 'new', 'queer', 'circle', 'space', 'strength', 'lives', 'hanging', 'something', 'need', 'feet', 'awoke', 'whoever', 'waiting', 'sir', 'rage', 'temple', 'air', 'bench', 'early', 'stick', 'fear', 'sorrow', 'feel', 'soul', 'stout', 'shoot', 'nurse', 'convinced', 'thanksgiving', 'daughter', 'king', 'service', 'thousand', 'opening', 'shaking', 'flower', 'opportunity', 'minutes', 'ago', 'real', 'probably', 'gets', 'near', 'simply', 'ended', 'smiling', 'someone', 'sister', 'sum', 'whilst', 'enough', 'crying', 'writing', 'heart', 'silent', 'addressed', 'marianne', 'occasion', 'pity', 'forehead', 'west', 'u', 'somehow', 'marble', 'mark', 'faithful', 'mole', 'dream', 'bow', 'slight', 'brass', 'find', 'anything', 'boys', 'fingers', 'weak', 'wanted', 'caused', 'across', 'fed', 'side', 'outside', 'persons', 'grief', 'bushes', 'excitement', 'softly', 'charming', 'paused', 'eyes', 'foolish', 'oak', 'even', 'interrupted', 'finding', 'forest', 'buy', 'johnny', 'ten', 'lips', 'back', 'smiled', 'nights', 'muttered', 'walls', 'fifteen', 'hast', 'shame', 'thick', 'enemy', 'gave', 'thrown', 'task', 'knowledge', 'cold', 'certainly', 'like', 'shadows', 'enjoy', 'wine', 'pig', 'far', 'darling', 'fellows', 'pursued', 'snake', 'wendy', 'turn', 'looking', 'worthy', 'slightest', 'toto', 'striking', 'slept', 'throne', 'forward', 'hearts', 'slipped', 'figure', 'shelter', 'direction', 'attention', 'question', 'handed', 'treat', 'beg', 'runs', 'candle', 'fourth', 'interesting', 'bag', 'huge', 'hut', 'worse', 'knocked', 'years', 'magnificent', 'different', 'spend', 'whose', 'sound', 'miss', 'lit', 'small', 'lad', 'burn', 'wish', 'general', 'necessary', 'fields', 'clever', 'beasts', 'serious', 'sleeping', 'fixed', 'please', 'use', 'murmured', 'saint', 'went', 'throw', 'commanded', 'four', 'fresh', 'ball', 'giving', 'flung', 'arrived', 'sighed', 'carried', 'french', 'arrival', 'afraid', 'burnt', 'hung', 'exactly', 'cloud', 'face', 'calm', 'perfect', 'mean', 'er', 'robbers', 'strangely', 'rushed', 'hard', 'another', 'merchant', 'girls', 'continually', 'lizabetha', 'sack', 'milk', 'lord', 'common', 'window', 'fur', 'round', 'honour', 'ass', 'fight', 'human', 'recognized', 'nine', 'makes', 'dressed', 'suddenly', 'waved', 'double', 'avoid', 'seems', 'evgenie', 'morning', 'string', 'impression', 'conversation', 'glancing', 'middle', 'eleven', 'brother', 'seen', 'neck', 'search', 'weeks', 'started', 'stood', 'sides', 'dare', 'painted', 'miller', 'art', 'knowing', 'wash', 'midnight', 'became', 'fire', 'agreeable', 'soldier', 'blow', 'family', 'minute', 'coat', 'stop', 'beast', 'joe', 'true', 'others', 'robin', 'able', 'walked', 'shook', 'part', 'hat', 'wishes', 'peasant', 'people', 'ugly', 'jungle', 'surprise', 'evidently', 'board', 'closed', 'sing', 'loved', 'sang', 'rolled', 'crowd', 'serve', 'moments', 'immense', 'happy', 'anyone', 'stole', 'therefore', 'sheep', 'often', 'bank', 'hundred', 'meat', 'possessed', 'chase', 'remembered', 'pack', 'horror', 'stranger', 'heavily', 'wicked', 'mother', 'voyage', 'glance', 'dozen', 'visit', 'opinion', 'constantly', 'deck', 'single', 'holding', 'whales', 'enchanted', 'care', 'gods', 'considerable', 'food', 'beating', 'silver', 'colonel', 'feed', 'loves', 'become', 'faces', 'events', 'worst', 'using', 'rare', 'pine', 'grew', 'dinner', 'mud', 'driving', 'oil', 'purple', 'suffer', 'letter', 'narrow', 'sounds', 'drop', 'wished', 'sick', 'blood', 'mice', 'tiger', 'companion', 'every', 'noble', 'escape', 'lake', 'make', 'approached', 'aside', 'remarked', 'secret', 'sooner', 'ready', 'felt', 'tower', 'longing', 'spoke', 'bear', 'fifty', 'growing', 'visited', 'long', 'christmas', 'several', 'past', 'stones', 'greatly', 'born', 'frog', 'obliged', 'ere', 'joined', 'business', 'fine', 'dust', 'folks', 'beautiful', 'worn', 'holmes', 'wake', 'touch', 'twenty', 'glad', 'grand', 'enjoyed', 'satisfied', 'finds', 'valuable', 'less', 'uttered', 'straight', 'respect', 'mowgli', 'points', 'wind', 'thus', 'life', 'can', 'golden', 'branch', 'ships', 'afternoon', 'wondering', 'know', 'steady', 'change', 'hare', 'fancy', 'rapidly', 'proceeded', 'nose', 'helped', 'killed', 'expected', 'train', 'delight', 'de', 'feathers', 'tom', 'flying', 'midst', 'showing', 'observed', 'changed', 'chosen', 'dear', 'enormous', 'setting', 'lines', 'likewise', 'lamp', 'seldom', 'condition', 'listening', 'saying', 'equally', 'speak', 'cup', 'quietly', 'failed', 'came', 'surprised', 'captain', 'comrade', 'fairly', 'prevent', 'expression', 'continued', 'reached', 'gold', 'winged', 'thee', 'entirely', 'youth', 'waste', 'laurie', 'companions', 'picked', 'thin', 'pushed', 'finish', 'rock', 'stopped', 'seated', 'occurred', 'nastasia', 'drink', 'children', 'hind', 'broke', 'voices', 'covered', 'history', 'seeing', 'seem', 'paper', 'wife', 'half', 'breath', 'fairies', 'eating', 'flat', 'fastened', 'increased', 'exclaimed', 'possession', 'mary', 'hungry', 'assured', 'frequently', 'watched', 'lights', 'hearty', 'anywhere', 'sleep', 'bones', 'daily', 'son', 'trunk', 'promised', 'answer', 'marry', 'fit', 'first', 'leaned', 'straw', 'safely', 'luck', 'making', 'nice', 'handkerchief', 'take', 'epanchin', 'silently', 'breaking', 'thirty', 'open', 'huntsman', 'boy', 'swift', 'stared', 'nature', 'lorry', 'poured', 'tied', 'intended', 'delivered', 'wide', 'town', 'public', 'rang', 'mounted', 'horses', 'lies', 'concealed', 'made', 'disturbed', 'chief', 'speaking', 'north', 'spent', 'instantly', 'girl', 'goes', 'snow', 'main', 'expressed', 'tried', 'birds', 'completely', 'loudly', 'loose', 'bed', 'ate', 'called', 'blessed', 'begged', 'home', 'couple', 'shadow', 'bigger', 'queequeg', 'pitcher', 'bits', 'content', 'says', 'desire', 'sisters', 'mount', 'falling', 'dress', 'glimpse', 'march', 'anxiety', 'meal', 'command', 'kindness', 'dirty', 'war', 'beauty', 'sailed', 'worth', 'careful', 'accept', 'allow', 'complete', 'idle', 'travelling', 'ye', 'going', 'assure', 'suffering', 'spirit', 'woodman', 'particularly', 'harm', 'village', 'ear', 'pretty', 'image', 'wandering', 'honest', 'letters', 'stopping', 'cave', 'seeking', 'dead', 'set', 'tale', 'hunger', 'high', 'awake', 'sending', 'london', 'brothers', 'broad', 'dancing', 'farmer', 'smell', 'peaceful', 'breakfast', 'calls', 'also', 'gentleman', 'questions', 'inside', 'stolen', 'count', 'famous', 'regard', 'broken', 'speech', 'end', 'plunged', 'unable', 'gania', 'wonderful', 'shake', 'second', 'large', 'now', 'gladly', 'pair', 'clean', 'mistress', 'gather', 'watching', 'rogojin', 'pan', 'afterwards', 'deadly', 'supposed', 'signs', 'eye', 'funny', 'unfortunate', 'choose', 'troubled', 'bold', 'altogether', 'used', 'shall', 'trees', 'sorts', 'flight', 'twisted', 'crossed', 'guessed', 'forth', 'fast', 'hearing', 'owl', 'brown', 'pointing', 'well', 'fetched', 'throwing', 'love', 'screamed', 'haste', 'sara', 'least', 'opened', 'race', 'takes', 'dashwood', 'sharp', 'prince', 'taste', 'served', 'will', 'satisfaction', 'beyond', 'giant', 'wandered', 'shot', 'sign', 'household', 'capital', 'perhaps', 'meant', 'corn', 'duty', 'advice', 'informed', 'managed', 'regarded', 'spot', 'servants', 'book', 'ladies', 'dashed', 'mouth', 'yesterday', 'scene', 'floating', 'axe', 'offer', 'swiftly', 'feelings', 'power', 'hill', 'look', 'age', 'gentlemen', 'thoughts', 'lively', 'never', 'stairs', 'go', 'roses', 'fortune', 'knows', 'doubt', 'living', 'scattered', 'reply', 'draw', 'immediately', 'considering', 'engaged', 'sunday', 'brilliant', 'cat', 'learn', 'suspected', 'met', 'quiet', 'women', 'dry', 'majesty', 'forgotten', 'tailor', 'looks', 'saved', 'startled', 'pray', 'father', 'friendly', 'conduct', 'astonished', 'lump', 'passed', 'begin', 'manners', 'caught', 'child', 'upon', 'plainly', 'widow', 'provided', 'boats', 'starting', 'pot', 'manage', 'likely', 'besides', 'dance', 'solemn', 'creeping', 'curious', 'opposite', 'account', 'particular', 'written', 'burst', 'ring', 'required', 'till', 'start', 'ill', 'journey', 'settle', 'answered', 'unknown', 'yards', 'learnt', 'asleep', 'jump', 'wonder', 'strong', 'dollars', 'beheld', 'breast', 'story', 'terror', 'dull', 'expect', 'easily', 'presently', 'marked', 'swept', 'try', 'judge', 'bringing', 'carry', 'hills', 'lonely', 'american', 'happened', 'sit', 'country', 'length', 'wrong', 'wings', 'valley', 'aware', 'times', 'spite', 'lest', 'instead', 'oh', 'storm', 'feeling', 'royal', 'arrow', 'terrible', 'naturally', 'wore', 'intend', 'hide', 'whence', 'east', 'cap', 'yet', 'toward', 'floated', 'noticed', 'course', 'whale', 'pain', 'pride', 'canst', 'desired', 'surely', 'dragged', 'aid', 'magic', 'determined', 'lost', 'dried', 'entered', 'save', 'grass', 'oz', 'stir', 'break', 'really', 'blind', 'act', 'built', 'keeps', 'produced', 'although', 'tone', 'suit', 'order', 'emperor', 'lodge', 'bit', 'calling', 'bitter', 'cry', 'sympathy', 'extremely', 'kid', 'laid', 'heads', 'giants', 'wrapped', 'teeth', 'get', 'wild', 'talked', 'die', 'effect', 'church', 'strike', 'fruit', 'thanked', 'knew', 'respectable'}\n",
    "\n",
    "    return len(filtered_words_set.intersection(SEVEN_PREC_MOST_FREQ_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0f0a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _diversity(filtered_words, filtered_words_set):\n",
    "    \"\"\"\n",
    "    Fraction of unique words from the total number of words (exclusing stop words).\n",
    "    Higher is more diversified.\n",
    "    Args:\n",
    "        filtered_words (list): set of non-stop tokenized words. \n",
    "        filtered_words_set (set): unique filtered words.\n",
    "    \"\"\"\n",
    "    MIN_WORDS_PER_STORY = 5\n",
    "    # If empty sentence or only white space or \\n or too repetitive.\n",
    "    if len(filtered_words_set) < MIN_WORDS_PER_STORY:\n",
    "        return 0\n",
    "\n",
    "    return len(filtered_words_set) / len(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2071ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import KLDivLoss, Softmax, LogSoftmax\n",
    "\n",
    "def _load_KLDIV_loss_function(device):\n",
    "    \"\"\"\n",
    "    Load loss function and its utilities.\n",
    "    \"\"\"\n",
    "    loss_fct = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "    softmax = Softmax(dim=-1)\n",
    "    logSoftmax = LogSoftmax(dim=-1)\n",
    "    loss_fct.to(device)\n",
    "    softmax.to(device)\n",
    "    logSoftmax.to(device)\n",
    "    return loss_fct, softmax, logSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1adce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLDIV_error_per_text(tokenizer, preset_model, finetuned_model, text):\n",
    "    \"\"\"\n",
    "    Computes the difference in prediction scores of the given text between the two models.\n",
    "    The preset_model scores are the input distribution and the finetuned_model scores are the target distribution.\n",
    "    The forward pass of each model returns the \"prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\" (from: https://huggingface.co/transformers/model_doc/gpt2.html).\n",
    "\n",
    "    Args:\n",
    "        tokenizer (Pytroch tokenizer): GPT2 Byte Tokenizer. \n",
    "        preset_model (Pytorch model): preset GPT2 model of the same/ different size of the finetuned model. Assumes model max_length < num_of_words(text).\n",
    "        finetuned_model (Pytorch model): fine-tuned GPT2 model. Assumes model max_length < num_of_words(text).\n",
    "        text (str): generated text to check predictions scores for.\n",
    "    Returns:\n",
    "        float representing the difference in scores. Bigger is probably better since it usually means the text is closer to the finetuned distribution.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Text is divided to extracts e.g. start, middle, end.\n",
    "    if isinstance(text, (list, np.ndarray)):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    # if too short return 0\n",
    "    if len(text) < 10:\n",
    "        return 0\n",
    "\n",
    "    # Prepare text to forward pass.\n",
    "    encodings_dict = tokenizer(text)\n",
    "    text_ids = torch.tensor(\n",
    "        encodings_dict['input_ids'], device=device, dtype=torch.long)\n",
    "\n",
    "    # Load loss function, the losses are averaged over batch dimension.\n",
    "    loss_fct, softmax, logSoftmax = _load_KLDIV_loss_function(device)\n",
    "    # unsqueeze to add batch_size =1\n",
    "    # zero index to logits, logits shape (batch_size, len(text_ids), config.vocab_size)\n",
    "    logists_preset = preset_model(text_ids.to(device).unsqueeze(0))[0]\n",
    "    logits_finetuned = finetuned_model(text_ids.to(device).unsqueeze(0))[0]\n",
    "\n",
    "    # input should be in log-probabilities, and target in probabilities.\n",
    "    return loss_fct(logSoftmax(logists_preset), softmax(logits_finetuned)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c80b008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (from scikit-learn) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (from scikit-learn) (3.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tfede\\Documents\\GitHub\\Master-thesis\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a6bd9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coherency(texts_sentences):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        texts_sentences (list): one story text split to sentences.\n",
    "        lsa (sklearn Vectorizer).\n",
    "    Based on LSA (TF-IDF -> Truncated SVD), returns the similarity within the story setences in comparison to the first sentence.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # Compute tf-idf per extract.\n",
    "#     transformed_sentences = embedder(texts_sentences)\n",
    "    transformed_sentences = vectorizer.fit_transform(texts_sentences)\n",
    "    # Compute cosine max similarity per extract, shape (#texts_sentences x #texts_sentences).\n",
    "    similarity = cosine_similarity(transformed_sentences)\n",
    "    # Compute similarity scores with first sentence of the rest of the sentences.\n",
    "    return sum(similarity[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2042226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\").cuda()\n",
    "device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sentence = [\"I want to be a millionaire \"]\n",
    "first_idx = False\n",
    "encodings_dict = tokenizer(sentence)\n",
    "sliced_inputs = [encodings_dict['input_ids'][0][-550:]]\n",
    "\n",
    "prompts_ids = torch.tensor(\n",
    "    sliced_inputs, device=device, dtype=torch.long)\n",
    "first_idx = len(prompts_ids[0]) if first_idx else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d8668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_generated_text(sample, tokenizer, has_space):\n",
    "    decoded = tokenizer.decode(\n",
    "        sample, skip_special_tokens=True)\n",
    "    # Removing spaces.\n",
    "    decoded = decoded.strip()\n",
    "    # Adding a space at the beginning if needed.\n",
    "    if not has_space:\n",
    "        decoded = ' ' + decoded\n",
    "    # Filtering � globally\n",
    "    return re.sub(u'\\uFFFD', '', decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "639206de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I want to be a millionaire'\n",
      " 'I want to be a millionaire!!! I am from New York and  I am a member of the family!  My'\n",
      " 'I want to be a millionaire!\"\\n\\n\"If I do that... well, that\\'s not a bad job.'\n",
      " 'I want to be a millionaire \\nI want to get rich'\n",
      " 'I want to be a millionaire  (don\\'t ever call him \\'pro,\\'\"  (silly) but, when'\n",
      " 'I want to be a millionaire!!! and get a home life with an art background. And I am willing to be rich'\n",
      " 'I want to be a millionaire  (as  I am at \\nthe present time). My  wish - as'\n",
      " 'I want to be a millionaire?\" \" Yes.\" \" I think that\\'s a good way?\" \" So you\\'re'\n",
      " 'I want to be a millionaire \\n\\nI read several books about buying a house \\n\\nWhat are the major advantages'\n",
      " 'I want to be a millionaire   \\nso that I have a life.   \\nMy money']\n"
     ]
    }
   ],
   "source": [
    "max_length = 25\n",
    "num_return_sequences=10\n",
    "sample_outputs = model.generate(\n",
    "    prompts_ids,  # Long tensor of size (batch_size, max_prompt_length)\n",
    "    do_sample=True,  # activate top-k, top-p sampling\n",
    "    max_length=max_length+first_idx,\n",
    "    min_length=first_idx + max_length//2 if first_idx else 10,\n",
    "    top_k=70,\n",
    "    top_p=0.95,\n",
    "    temperature=1.05,\n",
    "    repetition_penalty=1.0,  # no penalty\n",
    "    num_return_sequences=num_return_sequences,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")  # returns tensor of shape (len(prompts)*num_return_sequences x max_length)\n",
    "has_space = sentence[0][-1].isspace()\n",
    "generated = map(lambda sample: _preprocess_generated_text(\n",
    "    sample[first_idx:], tokenizer, has_space), sample_outputs)\n",
    "generated = np.array(\n",
    "    list(filter(lambda sample: len(sample.strip()) > 2, generated)))\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "417805de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stop-words in c:\\users\\tfede\\documents\\github\\master-thesis\\venv\\lib\\site-packages (2018.7.23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tfede\\Documents\\GitHub\\Master-thesis\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install stop-words\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "STOP_WORDS = set(get_stop_words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65e46c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_text(text, tokenizer, preset_model, finetuned_model):\n",
    "# def score_text(text):\n",
    "    \"\"\" Uses rule-based rankings. Higher is better, but different features have different scales.\n",
    "\n",
    "    Args:\n",
    "        text (str/ List[str]): one story to rank.\n",
    "        tokenizer (Pytroch tokenizer): GPT2 Byte Tokenizer. \n",
    "        preset_model (Pytorch model): preset GPT2 model of the same/ different size of the finetuned model. \n",
    "        finetuned_model (Pytorch model): fine-tuned GPT2 model. \n",
    "\n",
    "    Returns a scores np.array of corresponding to text.\n",
    "    \"\"\"\n",
    "    assert isinstance(\n",
    "        text, (str, list)), f\"score_text accepts type(text) = str/list, but got {type(text)}\"\n",
    "\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    # Keep same order as in constants.FEATURES\n",
    "    scores = [0 for _ in range(6)]\n",
    "    texts_sentences = split_to_sentences(text)\n",
    "    scores[0] = _coherency(texts_sentences)\n",
    "    scores[1] = _readabilty(text, texts_sentences)\n",
    "\n",
    "#     Set of text words without punctuation and stop words.\n",
    "    filtered_words = list(filter(\n",
    "        lambda word: word not in STOP_WORDS, split_words(text.lower().strip())))\n",
    "    filtered_words_set = set(filtered_words)\n",
    "    # Sentiment.\n",
    "    scores[2] = _sentiment_polarity(filtered_words)\n",
    "\n",
    "    # Set based measures.\n",
    "    scores[3] = _simplicity(filtered_words_set)\n",
    "    scores[4] = _diversity(filtered_words, filtered_words_set)\n",
    "\n",
    "#     # The bigger differene, the more tale-like, similar to the fine-tuned model, the text is.\n",
    "    scores[5] = KLDIV_error_per_text(\n",
    "        tokenizer, preset_model, finetuned_model, text)\n",
    "\n",
    "#     # print(\" | \".join(f'{key}: {score:.2f}' for key,\n",
    "#     #                  score in zip(constants.FEATURES, scores)))\n",
    "\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3c1ce22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_scores(stories_scores):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        stories_scores (np.array): 2D matrix of shpae (#stories x #ranking_features).\n",
    "    Returns the indices of top stories accroding to scores, from highest to lowest (descending).\n",
    "    \"\"\"\n",
    "    # Rescale each feature column across all stories, so that all featues contribute equally.\n",
    "    # stories_scores_std = (stories_scores - np.mean(stories_scores,axis=0))/np.std(stories_scores,axis=0)\n",
    "    stories_scores_normalized = stories_scores - \\\n",
    "        np.min(stories_scores, axis=0)\n",
    "    min_max_denominator = np.max(\n",
    "        stories_scores, axis=0) - np.min(stories_scores, axis=0)\n",
    "    # Avoid devision by zero, out to initialize idices where denominator ==0.\n",
    "    stories_scores_normalized = np.divide(\n",
    "        stories_scores_normalized, min_max_denominator, out=np.zeros_like(stories_scores_normalized), where=min_max_denominator != 0)\n",
    "\n",
    "    # Sort by mean story score, shape: (num_stories)\n",
    "    return np.argsort(np.mean(stories_scores_normalized, axis=1))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9fd23085",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cosine_similarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22308\\1552347739.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m stories_scores = np.array(list(map(lambda text: score_text(\n\u001b[1;32m----> 2\u001b[1;33m     text, tokenizer, model, model), generated)))\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mstories_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22308\\1552347739.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m stories_scores = np.array(list(map(lambda text: score_text(\n\u001b[1;32m----> 2\u001b[1;33m     text, tokenizer, model, model), generated)))\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mstories_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22308\\266320836.py\u001b[0m in \u001b[0;36mscore_text\u001b[1;34m(text, tokenizer, preset_model, finetuned_model)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mtexts_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_to_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_coherency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_readabilty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22308\\962067991.py\u001b[0m in \u001b[0;36m_coherency\u001b[1;34m(texts_sentences)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtransformed_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Compute cosine max similarity per extract, shape (#texts_sentences x #texts_sentences).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0msimilarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Compute similarity scores with first sentence of the rest of the sentences.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cosine_similarity' is not defined"
     ]
    }
   ],
   "source": [
    "stories_scores = np.array(list(map(lambda text: score_text(\n",
    "    text, tokenizer, model, model), generated)))\n",
    "stories_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc5918da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 4, 7, 8, 6, 2, 9, 3, 0], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_idx = sort_scores(stories_scores)\n",
    "sorted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe66a3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I want to be a millionaire!!! and get a home life with an art background. And I am willing to be rich',\n",
       " 'I want to be a millionaire?\" \" Yes.\" \" I think that\\'s a good way?\" \" So you\\'re',\n",
       " 'I want to be a millionaire!!! I am from New York and  I am a member of the family!  My',\n",
       " 'I want to be a millionaire  (don\\'t ever call him \\'pro,\\'\"  (silly) but, when',\n",
       " 'I want to be a millionaire  (as  I am at \\nthe present time). My  wish - as',\n",
       " 'I want to be a millionaire \\n\\nI read several books about buying a house \\n\\nWhat are the major advantages',\n",
       " 'I want to be a millionaire   \\nso that I have a life.   \\nMy money',\n",
       " 'I want to be a millionaire!\"\\n\\n\"If I do that... well, that\\'s not a bad job.',\n",
       " 'I want to be a millionaire \\nI want to get rich',\n",
       " 'I want to be a millionaire']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(generated[sorted_idx])[:num_return_sequences]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
